{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data science workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](imgs/data_science_workflow.png)\n",
    "\n",
    "## We are going to focus on the \"Wrangle->Clean->Explore\" part.  \n",
    "- \"Data wrangling:\" cleaning and unifying messy/complex data sets for easy access and analysis\n",
    "- The majority of a data scientists time is spent on data cleaning/wrangling!\n",
    "- \"Garbage in/garbage out:\"  Incorrect or poor quality input will always produce faulty output.  Examples:\n",
    "    - Some rows in a table are duplicated\n",
    "    - Some data entries are outside the realistic range (e.g. negative values for age)\n",
    "    - Data combined from two sources uses two different scales (e.g. celsius vs. Farenheit)\n",
    "\n",
    "![pic](imgs/forbes_data_sci.jpg)\n",
    "[Source](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#96b939c6f637)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Setup - installing pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this pandas tutorial, we'll need to make sure we have access to:\n",
    "- python3\n",
    "- pandas library\n",
    "- Jupyter notebooks\n",
    "\n",
    "You have a few options:\n",
    "1. Cloud environment (start here if you're not command line savvy and/or haven't used jupyter notebooks before): \n",
    "    - Use the JupyterLab binder button on the github repo. All of the dependencies are included in the environment.yml file used to build the binder container; you do not need to install anything on your computer.\n",
    "2. Local environment, option 1: \n",
    "    - Clone the repo (`git clone https://github.com/marskar/snakemake.git`)\n",
    "    - Use conda (or your favorite package manager) to install the dependencies locally (`conda install -c bioconda <tool=version>`)\n",
    "3. Local environment, option 2:\n",
    "    - Clone the repo (`git clone https://github.com/marskar/snakemake.git`)\n",
    "    - Use the environment file to build a conda environment locally (`conda env create -f environment.yml`)\n",
    "    - Be sure to start up jupyter notebook from within this environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  About pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is pandas?\n",
    "\n",
    "- Package for cleaning, transforming, and analyzing your data\n",
    "- Built on top of NumPy, with additional functionality built in to deal with messier data\n",
    "    - Attach row and column labels\n",
    "    - Work with missing data\n",
    "    - Handle heterogeneous data types\n",
    "    \n",
    "## Why use pandas?\n",
    "\n",
    "- Have you ever used an Excel spreadsheet?  What were some of the ways you gleaned insights from your data?\n",
    "\n",
    "- Using pandas to clean and explore your data can be followed by application of machine learning models, or it can be a standalone analysis, depending on your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two central components in pandas are the `series` and the `dataframe`.  A `series` is a column of data, and a `dataframe` is a table made up of a group of `series`.  Visually:\n",
    "\n",
    "__Series:__\n",
    "\n",
    "|Age|\n",
    "|---|\n",
    "|3|\n",
    "|7|\n",
    "|18|\n",
    "|54|\n",
    "|12|\n",
    "\n",
    "__Dataframe:__\n",
    "\n",
    "|Age|Sex|Height|\n",
    "|---|---|---|\n",
    "|3|M|36|\n",
    "|7|F|45|\n",
    "|18|F|60|\n",
    "|54|M|66|\n",
    "|12|M|58|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the numpy and pandas libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Building series and dataframes from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's build a couple series from numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_array = np.array([3,7,18,54,12])\n",
    "age_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_series = pd.Series(age_array)\n",
    "age_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_array = np.array(['M','F','F','M','M'])\n",
    "sex_series = pd.Series(sex_array)\n",
    "sex_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a dataframe from scratch.  There are many ways to do this.  Let's first build a dataframe from a multidimensional numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.array([[3,7,18,54,12], ['M','F','F','M','M']]).T)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could build a dataframe from scratch using a dict.  A dict is composed of __key:value__ pairs (in this case, the keys are age and sex; each has a single value which is a python list).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'age': [3,7,18,54,12], \n",
    "    'sex': ['M','F','F','M','M']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's pretty cool, but it's unlikely that you'll want to manually type in your whole dataset as an array or a dict.  How can we read external data into a dataframe?  Let's say we have a tab-separated text file with data from an esophageal cancer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('data/esophageal_subjects.txt', sep='\\t')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this notebook in Google Colab, use the following cell to pull in the data (this will also work in Binder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/bballew/pandas_tutorial/master/data/esophageal_subjects.txt'\n",
    "df_orig = pd.read_csv(url, sep='\\t')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  Viewing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: First, put the cursor in the following cell, then press `shift + TAB`.  If that does nothing, prepend the cell with a '?' and run it.  Depending on your environment, one of these two options will pull up documentation.  Then, edit the cell to view the first 10 lines of the dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data types reported above.  For example, 'C3_EDUC' looks like a column of numbers, but the dtype is reported as \"object\", which is a string.  This is a clue that something might be awry in this column (for example, there may be a missing value that is noted with a string like 'na')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count non-null values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count distinct values by column.  Check here that columns have the expected number of unique values, e.g. two values for sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  Clean up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.  Remove duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to check your data for duplicate entries.  Using `drop_duplicates()`, we can remove all duplicate rows, or keep only the first or last row of a set of duplicates.  Here, we remove a row in which all values are duplicates of another row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_orig.copy()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sort_values(\"ID\", inplace = True)\n",
    "df_clean.drop_duplicates(keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove duplicates based on specific values, like \"Dx\".  In this example, we keep only the last row of each duplicate diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Dx = df_clean.sort_values('Dx')\n",
    "df_Dx.drop_duplicates(subset = 'Dx', keep = 'last', inplace = True)\n",
    "df_Dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Copy the dataframe df_orig, sort the copy by sex, drop all but the first duplicates, and determine the shape__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.  Modify column headings and row labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column headings in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns headings on our imported data are difficult to interpret.  Let's change them to something that makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.rename(columns={'C3_EDUC':'Education_level',\n",
    "                   'C16_HFUEL':'Heating_source',\n",
    "                   'D1_EVCIG':'Smoke_6mos',\n",
    "                   'D2_START':'Age_start_smoking',\n",
    "                   'D4_STOP':'Age_stop_smoking',\n",
    "                   'D5_NUMCIG':'Cigarettes_per_day',\n",
    "                   'D8_LIQ':'Liquor_freq',\n",
    "                   'E1_UGICAN':'Family_hx_UGIcancer',\n",
    "                   'G1_TEMP':'Temp_of_hot_beverages',\n",
    "                   'G2_SFREQ':'Frequency_scalding_food',\n",
    "                   'H1_HEIGHT':'Height_cm',\n",
    "                   'H2_WEIGHT':'Weight_kg'},\n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also change the column headings using `.columns`.  Let's say we wanted a dataframe with only lowercase column headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns = ['id', 'dx', 'age', 'sex', 'education_level', 'heating_source',\n",
    "       'smoke_6mos', 'age_start_smoking', 'age_stop_smoking',\n",
    "       'cigarettes_per_day', 'liquor_freq', 'family_hx_ugicancer',\n",
    "       'temp_of_hot_beverages', 'frequency_scalding_food', 'height_cm',\n",
    "       'weight_kg', 'ps_id', 'wgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our row labels right now are just numbered, i.e. 0,1,2...  Let's change the row labels to the patient IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.  Re-coding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has many encoded values, e.g. integers that represent various levels of education, heating fuel types, etc.  Let's replace the encoded data with the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['education_level'].replace(['1','2','3','4','5','6','7'],['None','1-5yrs','Primary','Middle','High','College','Other'], inplace=True)\n",
    "df_clean['heating_source'].replace([1,2,3,4,5,6,7,8],['Coal','Coke','Coal_product','Firewood','Diesel','Kerosene','Gas','Other'], inplace=True)\n",
    "df_clean['smoke_6mos'].replace([1,2],['Yes','No'], inplace=True)\n",
    "df_clean['liquor_freq'].replace([1,2,3,4,5],['Daily','Weekly','Monthly','Seasonally','Rarely-never'], inplace = True)\n",
    "df_clean['family_hx_ugicancer'].replace(['1','2'],['Yes','No'], inplace = True)\n",
    "df_clean['temp_of_hot_beverages'].replace([1,2,3,4,9],['Cool','Warm','Hot','Scalding','DK'], inplace=True)\n",
    "df_clean['frequency_scalding_food'].replace([1,2,3,4,9],['Seldom-never','Occasionally','Weekly','Daily','DK'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the deal with `[1,2,3,]` vs. `['1','2','3']` above?  Recall the result of `.info()`.  Columns that contain only integers like 1,2,3 are of an integer dtype.  However, if there are mixed types within the column (e.g. a missing value like '.' or 'Nan'), then the column will be an object type, which are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Copy df_clean and replace 'M' and 'F' with 'male' and 'female', then show the first 5 rows__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.  Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets often have missing values.  These can be annotated in a variety of ways - 'None', 'NaN', 'na', '.', a blank space...  These must first be identified.  Then, we can either drop the relevant rows/columns, or replace the missing values with something (often the mean or median of the column).  Some pandas techniques will handle missing values by simply omitting them from the calculation, so depending on what you're doing, you may be able to tolerate some missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Use `shift + TAB` or prepend with a `?` to look at the docustring for `.sum()`.  How would we get the sum of null values in each row, instead of each column?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look.  What type of \"missing\" values does `isnull()` detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['education_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['education_level'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...`isnull()` doesn't recognize all of our missing values.  We'll need to address this by normalizing all the missing values that we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.replace(['None','.','--','na'],np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['education_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can detect all our missing values.  Let's deal with missing data for the smoking-related columns.  First, let's make sure that the missing data in 'age_start_smoking' and 'age_stop_smoking' isn't due to that subject being a non-smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean[['smoke_6mos','age_start_smoking','age_stop_smoking','cigarettes_per_day']].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most of the missing data in these columns is because the individual was not a smoker.  Let's replace the one valid missing data field with the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_mean = round(df_clean['age_stop_smoking'].mean())\n",
    "stop_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.at['PS002','age_stop_smoking']=stop_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[['PS002']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's impute the missing values for age by replacing with the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = round(df_clean['age'].mean())\n",
    "age_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['age'].fillna(age_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also opt to remove some data, either rows or columns, if there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop = df_clean.dropna() # drop rows\n",
    "df_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: We used the mean of 'age' to fill in missing values in df_clean.  What if we wanted to use the median?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.  Check for biologically unrealistic values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, a value isn't technically missing, but it's clearly outside realistic expectations.  Let's check for examples of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['height_cm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['weight_kg'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh!  It seems unlikely that someone would be 773 cm tall, or weigh 5 kg.  Look at a few quick plots to make sure there aren't other aberrant values like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['weight_kg'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['height_cm'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like those two are the only obviously incorrect values.  Let's replace them with the mean of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_mean = round(df_clean['height_cm'].mean())\n",
    "height_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['height_cm'].replace(773, height_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mean = round(df_clean['weight_kg'].mean())\n",
    "weight_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['weight_kg'].replace(5, weight_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-check the distributions, to make sure everything looks plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Take a closer look at the 'age' data by plotting a histogram.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6  Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done a lot of work cleaning our data, let's make sure each column contains the expected data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'cigarettes_per_day' is listed as object, which is a string type.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['cigarettes_per_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['cigarettes_per_day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN is a float-representation of missing data; the other entries in this column are integers.  Let's change them all to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['cigarettes_per_day'] = df_clean['cigarettes_per_day'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.  Slicing and extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen some of these techniques in action already.  You can subset a dataframe in several ways, and the subsetted data can be either a dataframe or a series.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's subset by column name.  Notice that this results in a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_clean['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can subset by column name and get a dataframe by passing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[['age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_clean[['age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily subset a dataframe by multiple columns, by adding column names to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[['age','weight_kg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract data from rows in the dataframe by index or by label, using `.loc` and `.iloc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc['PS002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with columns, the above methods give you a series.  You can get a dataframe by providing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[['PS002']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Look at the column headers in df_clean by using `.columns`, then select one to extract from the dataframe.  How do you extract the data as a series?  As a dataframe?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.  Filter and group data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can subset your dataframe using conditional selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what happens if we make a conditional selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['heating_source'] == 'Coal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply that selection to the dataframe to retrieve the relevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['heating_source'] == 'Coal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Retrieve all rows from df_clean where the heating source is anything other than 'Coal'.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select continuous variables in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['age'] > 65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply multiple criteria to your selection using or (`|`) or and (`&`).  Note the use of parentheses to group conditional statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[(df_clean['temp_of_hot_beverages'] == 'Scalding') | (df_clean['temp_of_hot_beverages'] == 'Hot')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.isin()` is more concise than the above command, but does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean['temp_of_hot_beverages'].isin(['Scalding', 'Hot'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an `&` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[(df_clean['temp_of_hot_beverages'] == 'Scalding') & (df_clean['frequency_scalding_food'] == 'Daily')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can group your data using `groupby()`.  For example, let's say we want to calculate the mean age for each category of \"temp_of_hot_beverages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['temp_of_hot_beverages'])['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Now that we know `.groupby()`, how could we have used it to check that non-smoking status ('smoke_6mos') correlates correctly with missing values for 'age_start_smoking', 'age_stop_smoking', and 'cigarettes_per_day?'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  Apply filters, groupby, etc. to generate insightful plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot what we just calculated, the mean age for each category of \"temp_of_hot_beverages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['temp_of_hot_beverages'])['age'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much do our subjects drink?  Let's use `.value_counts()` to determine how many instances there are of each \"liquor_freq\" category, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['liquor_freq'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['liquor_freq'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine filtering and plotting to look at the distribution of smokers' start and stop ages, amongst subjects who drink scalding beverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['temp_of_hot_beverages'] == 'Scalding'].boxplot(column=['age_start_smoking','age_stop_smoking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `groupby()` again, this time to look at the means for all continous variables, grouped by education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['education_level']).mean().plot.bar(figsize=[10,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a rather busy graph - let's redraw it, just focusing on smoking start/stop ages.  Do you think there's likely to be a difference in start/stop age across different levels of education?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['education_level'])['age_start_smoking','age_stop_smoking'].mean().plot.bar(figsize=[10,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create scatter plots to look at the interaction between multiple features.  Here, let's look at height and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.plot.scatter('height_cm','weight_kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `corr()` to look for correlations within our data.  Without any additional parameters, this will compare all continuous variables in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of ways to create a heatmap from this data.  Here's just one simple one, that doesn't rely on any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.corr().style.background_gradient(cmap='autumn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Generate a scatter plot of any two continuous variables.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: Plot the distribution of all continuous variables for subjects below 65 and above 65.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.  Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to download the notebook if you've made changes and want to save them!  If you re-open the binder link, it will be reset to the way you originally found it.\n",
    "\n",
    "Already use Excel, and want to start using pandas on your own?  Try this challenge: open your data in Excel and read it into pandas.  For every action you perform in Excel, do the equivalent thing in pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
